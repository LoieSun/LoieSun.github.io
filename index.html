<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Loie Sun</title>
  
  <meta name="author" content="Loie Sun">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:65%;vertical-align:middle">
              <p style="text-align:center">
                <name>Sun Luoyi</name>
		<br>
              </p>
              <p>I am a second-year master in <a href="https://sfa.shu.edu.cn/">Shanghai Film Academy</a>, <a href="https://www.shu.edu.cn/">Shanghai University</a>, supervised by Prof. Zhifeng Xie. 
		       I earned my Bachelor degree from <a href="http://www.sei.ynu.edu.cn/">School of Software</a>, <a href="http://www.ynu.edu.cn/">Yunnan University</a>.
		</p>
	      <p>
              My research interests include deep learning, cross-modal generation and music generation.
              </p>
              <p>
              I am actively seeking a PhD position.
              </p>
              <p style="text-align:center">
                <a href="Loie.pdf">CV</a> &nbsp/&nbsp
                <a href="sunluoyi@shu.edu.cn/"> Email: sunluoyi@shu.edu.cn </a> &nbsp/&nbsp
    		<a href="https://github.com/LoieSun"> GitHub </a> &nbsp/&nbsp
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/loie.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/loie.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
	
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
		
	   <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/sound_generation.png" alt="soundgeneration" width="160" height="100">
                </td>
                <td width="75%" valign="middle">
                  <a href="data/Sound Generation Method based on Timing-aligned Visual Feature Mapping.pdf">
                    <papertitle> Sound Generation Method based on Timing-aligned Visual Feature Mapping </papertitle>
                  </a>
                  <br>
		 Zhifeng Xie, <b>Luoyi Sun</b>, Yuzhou Sun, Chunpeng Yu, Lizhang Ma
                  <br>
                  <em>ChinaMM</em>, 2022 &nbsp 
                  <br>
                  <a href="data/Sound Generation Method based on Timing-aligned Visual Feature Mapping.pdf">pdf</a> 
                  <p></p>
                  <p>New framework for high-quality sound generation, matching to silent videos in content and timing alignment.</p>
                </td>
              </tr>
	     <tr>
		     
	     <tr>
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/SOD.png" alt="sod" width="160" height="100">
                </td>
                <td width="75%" valign="middle">
                  <a href="https://ieeexplore.ieee.org/abstract/document/9506100">
                    <papertitle> Multi-Scale Graph Convolutional Interaction Network For Salient Object Detection </papertitle>
                  </a>
                  <br>
		Wenqi Che, <b>Luoyi Sun</b>, Zhifeng Xie, Youdong Ding, Kanli Han
                  <br>
                  <em>ICIP</em>, 2021 &nbsp 
                  <br>
                  <a href="data/Multi-Scale_Graph_Convolutional_Interaction_Network_For_Salient_Object_Detection.pdf">pdf</a> 
                  <p></p>
                  <p>Proposed the multi-scale graph convolutional interaction network (MGCINet), and get the SOTA on five benchmark datasets. </p>
                </td>
              </tr>
	
         
  
              
              
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/chimpanzees.gif" alt="chimpanzees-facial-recognition" width="170" height="130">
            </td>
            <td width="75%" valign="middle">
              <a href="https://advances.sciencemag.org/content/5/9/eaaw0736">
                <papertitle>Chimpanzee face recognition from videos in the wild using deep learning</papertitle>
              </a>
              <br> Daniel Schofield*, <b>Arsha Nagrani*</b>, Andrew Zisserman, Misato Hayashi, Tetsuro Matsuzawa, Dora Biro, Susana Carvalho
              <br>
              <em>Science Advances</em>, 2019
              <br>
              <a href="https://www.robots.ox.ac.uk/~vgg/research/ChimpanzeeFaces/">project page</a> 
            </br>
              
		      Press:
		      <font color="orange">
				  <a href="https://www.newscientist.com/article/2215359-ai-facial-recognition-software-now-works-for-wild-chimpanzees-too/", style="color: orange">New Scientist</a>,
				  <a href="https://www.technologyreview.com/f/614260/ai-face-recognition-tracks-chimpanzees/", style="color: orange">MIT Tech Review</a>, 
				  <a href="https://techxplore.com/news/2019-09-artificial-intelligence-primate-wild.html", style="color: orange">TechXplore</a>, 
				  <a href="https://www.verdict.co.uk/chimpanzees-facial-recognition/", style="color: orange">Verdict</a>, 
				  <a href="https://www.digitaltrends.com/cool-tech/facial-recognition-chimpanzee/", style="color: orange">Digital Trends</a>, 
				  <a href="https://www.eng.ox.ac.uk/news/artificial-intelligence-used-to-recognise-primate-faces-in-the-wild/", style="color: orange">Oxford News</a> 
			  </font>
			  <br>
              <p></p>
              <p>Face detection, tracking, and recognition of wild chimpanzees from long-term video records using deep CNNs. We also show a brief application for social network analysis.
			  </p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/TBW.png" alt="clean-usnob" width="170" height="140">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/1908.08498.pdf">
                <papertitle>EPIC-Fusion: Audio-Visual Temporal Binding for Egocentric Action Recognition</papertitle>
              </a>
              <br>
			  Evangelos Kazakos, <b>Arsha Nagrani</b>, Andrew Zisserman, Dima Damen
              <br>
              <em>ICCV</em>, 2019
              <br>
			  <a href="https://ekazakos.github.io/TBN/">project page</a> /
              <a href="https://www.youtube.com/watch?time_continue=177&v=VzoaKsDvv1o">video</a> /
              <a href="https://github.com/ekazakos/temporal-binding-network">code and models</a> 
              <br>
              <p></p>
              <p>We propose a novel architecture for combining modalities in videos for action recognition, by using a temporal window to allow a range of temporal offsets.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/CE.png" alt="clean-usnob" width="170" height="150">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/1907.13487.pdf">
                <papertitle>Use What You Have: Video Retrieval Using Representations From Collaborative Experts</papertitle>
              </a>
              <br>
              Yang Liu*, Samuel Albanie*, <b>Arsha Nagrani*</b>, Andrew Zisserman
              <br>
              <em>BMVC</em>, 2019
              <br>
			  <a href="https://www.robots.ox.ac.uk/~vgg/research/collaborative-experts/">project page</a> /
              <a href="https://github.com/albanie/collaborative-experts">code & models</a> /
              <a href="https://www.robots.ox.ac.uk/~vgg/challenges/video-pentathlon/", style="color: orange"> challenge</a> 

              <p></p>
              <p>We fuse the information from different embeddings experts for the task of video retrieval - achieving SOTA results on 5 different datasets. This work is also the basis 
                for the <a href="https://www.robots.ox.ac.uk/~vgg/challenges/video-pentathlon/">Video Pentathlon</a> at CVPR 2020. 
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/spID.png" alt="clean-usnob" width="170" height="150">
            </td>
            <td width="75%" valign="middle">
              <a href="http://www.robots.ox.ac.uk/~vgg/publications/2019/Xie19a/xie19a.pdf">
                <papertitle>Utterance-level Aggregation For Speaker Recognition In The Wild  </papertitle>
              </a>
              <br>
			  Weidi Xie, <b>Arsha Nagrani</b>, Joon Son Chung, Andrew Zisserman 
              <br>
              <em>ICASSP</em>, 2019 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
              <br>
			  <a href="http://www.robots.ox.ac.uk/~vgg/research/speakerID/">project page</a> /
              <a href="https://github.com/WeidiXie/VGG-Speaker-Recognition">code & models</a> 
              <p></p>
              <p>A NetVlad layer in a deep CNN works well for speaker recognition on long noisy speech utterances.</p>
            </td>
          </tr>

          <tr onmouseout="acm_stop()" onmouseover="acm_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='acm_image'><img src='images/acm-after.png' alt="clean-usnob" width="170" height="150"></div>
                <img src='images/acm-before.png' alt="clean-usnob" width="170" height="150">
              </div>
              <script type="text/javascript">
                function acm_start() {
                  document.getElementById('acm_image').style.opacity = "1";
                }

                function acm_stop() {
                  document.getElementById('acm_image').style.opacity = "0";
                }
                acm_stop()
              </script>
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/1808.05561.pdf">
                <papertitle>Emotion Recognition in Speech using Cross-Modal Transfer in the Wild  </papertitle>
              </a>
              <br>
			  Samuel Albanie*, <b>Arsha Nagrani*</b>, Andrea Vedaldi, Andrew Zisserman
              <br>
              <em>ACM Multimedia</em>, 2018 
              <br>
              <a href="http://www.robots.ox.ac.uk/~vgg/research/cross-modal-emotions/">project page</a> /
              <a href="https://github.com/albanie/mcnCrossModalEmotions">code</a> 
              <p></p>
              <p>We use the redundant (common) signal in both audio (speech) and vision (faces) to learn speech representations for emotion recognition without manual supervision.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/vox2.png" alt="safs_small" width="170" height="120" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/1806.05622.pdf">
                <papertitle>VoxCeleb2: Deep Speaker Recognition </papertitle>
              </a>
              <br>
              Joon Son Chung*, <b>Arsha Nagrani*</b>, Andrew Zisserman 
              <br>
              <em>INTERSPEECH</em>, 2018
              <br>
			  <a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/">data</a> 
              <p>Speaker Recognition in the Wild using deep CNNs. The VoxCeleb datasets are also used integrally in the 
				  <a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/competition.html", style="color: orange"> VoxCeleb Speaker Recognition Challenge. </a></p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/jointembedding.png" alt="fast-texture" width="170" height="120">
            </td>
            <td width="75%" valign="middle">
              <a href="http://www.robots.ox.ac.uk/~vgg/publications/2018/Nagrani18c/nagrani18c.pdf">
                <papertitle>Learnable PINs: Cross-Modal Embeddings for Person Identity  </papertitle>
              </a>
              <br>
              <b>Arsha Nagrani*</b>, Samuel Albanie*, Andrew Zisserman 
              <br>
              <em>ECCV</em>, 2018
              <br>
              <a href="http://www.robots.ox.ac.uk/~vgg/research/LearnablePins/">project page</a> 
              <p>We learn joint embedding of faces and voices using cross-modal self-supervision from YouTube videos. 
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/seeingvoices.gif" alt="prl" width="170" height="120">
            </td>
            <td width="75%" valign="middle">
              <a href="http://www.robots.ox.ac.uk/~vgg/publications/2018/Nagrani18a/nagrani18a.pdf">
                <papertitle>Seeing Voices and Hearing Faces: Cross-modal biometric matching  </papertitle>
              </a>
              <br>
              <b>Arsha Nagrani</b>, Samuel Albanie, Andrew Zisserman
              <br>
              <em>CVPR</em>, 2018 &nbsp <font color="red"><strong>(Spotlight)</strong></font>
              <br>
              <a href="http://www.robots.ox.ac.uk/~vgg/research/CMBiometrics/">project page</a> / 
			  <a href="https://www.youtube.com/watch?time_continue=4&v=AJt993-VGsk"> video </a> / 
			  <a href="http://www.robots.ox.ac.uk/~vgg/blog/seeing-voices-and-hearing-faces.html"> blog post </a> 
              <p> Can you recognise someone’s face if you have only heard their voice? Or recognise their voice if you have only seen their face? </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/sherlock.gif" alt="blind-date" width="170" height="110">
            </td>
            <td width="75%" valign="middle">
              <a href="https://www.robots.ox.ac.uk/~vgg/publications/2017/Nagrani17b/nagrani17b.pdf">
                <papertitle>From Benedict Cumberbatch to Sherlock Holmes: Character Identification in TV series without a Script  </papertitle>
              </a>
              <br>
              <b>Arsha Nagrani</b>, Andrew Zisserman
              <br>
              <em>BMVC</em>, 2017 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
			  <br>
			  <a href="http://www.robots.ox.ac.uk/~vgg/research/Sherlock/">project page</a> 
              <p></p>
            </td>
          </tr>

	   <tr onmouseout="vox1_stop()" onmouseover="vox1_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='vox1_image'><img src='images/vox1-after.png' alt="clean-usnob" width="170" height="145"></div>
                <img src='images/voxceleb1.png' alt="clean-usnob" width="170" height="145">
              </div>
              <script type="text/javascript">
                function vox1_start() {
                  document.getElementById('vox1_image').style.opacity = "1";
                }

                function vox1_stop() {
                  document.getElementById('vox1_image').style.opacity = "0";
                }
                vox1_stop()
              </script>
            </td>
            <td width="75%" valign="middle">
              <a href="http://www.robots.ox.ac.uk/~vgg/publications/2017/Nagrani17/nagrani17.pdf">
                <papertitle>VoxCeleb: a large-scale speaker identification dataset  </papertitle>
              </a>
              <br>
              <b>Arsha Nagrani*</b>, Joon Son Chung*, Andrew Zisserman
              <br>
              <em>INTERSPEECH</em>, 2017 &nbsp <font color="red"><strong>(Oral Presentation, Best Student Paper Award)</strong></font>
			  <br>
			  <a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/">data</a> / 
			  <a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/competition.html">challenge</a> 
              <p>We use face recognition and active speaker detection to automatically create a large scale speaker identification dataset from YouTube videos.</p>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
        <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Teaching/Invited Talks</heading>
            <p>
            "Multimodality for Video Understanding", <a href="https://sites.google.com/view/aisummerschool2020">Google Research India AI Summer School</a>, 2020 [<a href="https://docs.google.com/presentation/d/1J4AVI9S1KBFZyGiBdFJn64k2BRFhlRRngd8r262Xo8g/edit?usp=sharing">slides</a>] </br>
            "Learning joint representations for visual and language tasks", <a href="https://multimodal-knowledge-discovery.github.io/">Online Multimodal Knowledge Discovery Tutorial</a>, ICDM 2020
            "Applications of Machine Learning", Oxford University MPLS DTC on Statistics and Data Mining, 2020 [<a href="https://docs.google.com/presentation/d/1XgNYFLNwlODst7LiFuaof2F9uuNcv3ZdwdiyXrCT1-A/edit?usp=sharing">slides</a>]
            </p>
          </td>
        </tr>
      </tbody></table>
    





        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Service</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <th style="padding:20px;width:25%;vertical-align:middle;text-align: center">
                      <img src="images/video-pent-logo.svg" alt="video-pent" width="100" height="80" class="center">
            </th>
           <td width="75%" valign="center">
            <strong> The End-of-End-to-End: </strong>  A Video Understanding Pentathlon @ CVPR 2020
            </br>
            <a href="https://arxiv.org/pdf/2008.00744.pdf">report</a> /
            <a href="https://www.robots.ox.ac.uk/~vgg/challenges/video-pentathlon/challenge.html">challenge</a> /
            <a href="https://www.robots.ox.ac.uk/~vgg/challenges/video-pentathlon/">workshop</a> 	/
            <a href="https://www.youtube.com/watch?v=L2rff2mu1gE">recording</a> 	
            </br>										 
            </br>
    
            </td>
            </tr>

          <tr>
        <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src="images/voxsrc.png" alt="voxsrc" width="150" height="50">
        </td>
       <td width="75%" valign="center">
	      <strong> VoxSRC: </strong> VoxCeleb Speaker Recognition Challenge @ INTERSPEECH 
        </br>
        [2021]
<a href="https://arxiv.org/pdf/2201.04583.pdf">report</a> / 	
	<a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/competition2021.html">challenge</a> /
	<a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/interspeech2021.html">workshop</a> /
        <a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/index.html">data</a>													              
       </br>
        [2020]
        <a href="https://arxiv.org/pdf/2012.06867.pdf">report</a> /
	<a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/competition2020.html">challenge</a> /
	<a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/interspeech2020.html">workshop</a> /
        <a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/index.html">data</a>													              
       </br>										 
        [2019]
        <a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/files/VoxSRC19.pdf">report</a> /
	      <a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/competition2019.html">challenge</a> /
	      <a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/interspeech2019.html">workshop</a> /
	      <a href="http://www.robots.ox.ac.uk/~vgg/data/voxceleb/index.html">data</a>													               </br>										 
	      </br>

        </td>
        </tr>

        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
                <img src="images/wicv.png" alt="wicv" width="150" height="50">
      </td>
      <td width="75%" valign="center">
        <strong> WICV: </strong> Women in Computer Vision Workshop @ CVPR
      </br>
      [2020]
      <a href="https://sites.google.com/view/wicvworkshop-cvpr2020/">website</a> /
      <a href="https://twitter.com/wicvworkshop?lang=en">twitter</a>													     									 
      </br>
        [2019]
	      <a href="http://openaccess.thecvf.com/content_CVPRW_2019/html/WiCV/Amerini_WiCV_2019_The_Sixth_Women_In_Computer_Vision_Workshop_CVPRW_2019_paper.html">report</a> /
	      <a href="https://wicvworkshop.github.io/CVPR2019/index.html">website</a> /
	      <a href="https://twitter.com/wicvworkshop?lang=en">twitter</a>													         </br>										 
        </br>
        </td>
        </tr>
        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle;text-align: center">
                <img src="images/reviewer.jpg" alt="review" width="150" height="70" >
      </td>
          <td>
              <strong> Reviewer </strong>: CVPR, ECCV, ICCV, BMVC, NeurIps, ICML, AAAI, IEEE Triple Access
              <br>

            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                 <a href="https://jonbarron.info/">This</a> guy is good at website design.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
