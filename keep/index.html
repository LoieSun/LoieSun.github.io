

<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<style type="text/css">
  @font-face {
   font-family: 'Avenir Book';
   src: url("./fonts/Avenir_Book.ttf"); /* File to be stored at your site */
   }

  body {
    font-family: "Avenir Book", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight:300;
    font-size:14px;
    margin-left: auto;
    margin-right: auto;
    width: 800px;
  }
  h1 {
    font-weight:300;
  }
  h2 {
    font-weight:300;
  }

  p {
    font-weight:300;
    line-height: 1.4;
  }

  code {
    font-size: 0.8rem;
    margin: 0 0.2rem;
    padding: 0.5rem 0.8rem;
    white-space: nowrap;
    background: #efefef;
    border: 1px solid #d3d3d3;
    color: #000000;
    border-radius: 3px;
  }

  pre > code {
    display: block;
    white-space: pre;
    line-height: 1.5;
    padding: 0;
    margin: 0;
  }

  pre.prettyprint > code {
    border: none;
  }


  .container {
        display: flex;
        align-items: center;
        justify-content: center
  }
  .image {
        flex-basis: 40%
  }
  .text {
        padding-left: 20px;
        padding-right: 20px;
  }
  .grid-container {
      display: grid;
      grid-template-columns: auto auto auto;
      grid-gap: 10px;
      background-color: #FFFFFF;
      padding: 10px;
}

  .disclaimerbox {
    background-color: #eee;
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
    padding: 20px;
  }

  video.header-vid {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.header-img {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.rounded {
    border: 0px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;

  }

  a:link,a:visited
  {
    color: #1367a7;
    text-decoration: none;
  }
  a:hover {
    color: #208799;
  }

  td.dl-link {
    height: 160px;
    text-align: center;
    font-size: 22px;
  }

  .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
            15px 15px 0 0px #fff, /* The fourth layer */
            15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
            20px 20px 0 0px #fff, /* The fifth layer */
            20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
            25px 25px 0 0px #fff, /* The fifth layer */
            25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
    margin-left: 10px;
    margin-right: 45px;
  }


  .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
    margin-top: 5px;
    margin-left: 10px;
    margin-right: 30px;
    margin-bottom: 5px;
  }

  .vert-cent {
    position: relative;
      top: 50%;
      transform: translateY(-50%);
  }

  hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }
</style>

  <title>KEEP</title>
</head>

<body data-new-gr-c-s-check-loaded="14.1093.0" data-gr-ext-installed="">
  <br>
  <center>
  <span style="font-size:32px">A Knowledge-enhanced Pathology Vision-language Foundation Model for Cancer Diagnosis</span><br><br><br>
  </center>
  <table align="center" width="625px">
            <tbody><tr>
                    <td align="center" width="625px">
              <center>
                <span style="font-size:16px">Xiao Zhou</a><sup>1</sup>, Luoyi Sun<sup>2,3</sup>, Dexuan He<sup>3</sup>, Wenbin Guan<sup>4</sup>, Ruifen Wang<sup>4</sup>, Lifeng Wang<sup>4</sup>,</span>
                Xin Sun<sup>5</sup>,</span>Kun Sun<sup>6</sup>,</span>Ya Zhang<sup>1,3</sup>,</span>Yanfeng Wang<sup>1,3*</sup>,</span>Weidi Xie<sup>1,3*</sup></span>
                </center>
                </td>

        
<!--                     
                    <td align="center" width="225px">
              <center>
                <span style="font-size:16px">Mengyue Wu</a><sup>2 <img class="round" style="width:20px" src="./resources/corresponding_fig.png"></sup></span>
                </center>
                </td>
                    <td align="center" width="225px">
              <center>
                <span style="font-size:16px"><a href="https://weidixie.github.io/">Weidi Xie</a><sup>1, 3 <img class="round" style="width:20px" src="./resources/corresponding_fig.png"></sup></span>
                </center>            
            
          </td> -->
        </tr>
        </tbody></table><br>
  
  <table align="center" width="700px">
            <tbody><tr>
                    <td align="center" width="50px">
              <center>
                    <span style="font-size:16px"></span>
                </center>
                </td>
                   
                    <td align="center" width="200px">
              <center>
                    <span style="font-size:16px"><sup>1</sup>Shanghai AI Lab</span>
                </center>
                </td>
                <td align="center" width="200px">
                  <center>
                        <span style="font-size:16px"><sup>2</sup> Zhejiang University</span>
                    </center>
                    </td>
                <td align="center" width="300px">
                  <center>
                        <span style="font-size:16px"><sup>3</sup> Shanghai Jiao Tong University</span>
                    </center>
                    </td>


        </tr></tbody></table>
        <table align="center" width="800px">
          <tbody><tr>
                  <td align="center" width="800px">
            <center>
                  <span style="font-size:16px"><sup>4</sup>Department of Pathology, Xin Hua Hospital Affiliated to Shanghai Jiao Tong University School of Medicine </span>
              </center>
              </td>
      </tr></tbody></table>

        </tr></tbody></table>
        <table align="center" width="850px">
          <tbody><tr>
                  <td align="center" width="850px">
            <center>
                  <span style="font-size:16px"><sup>5</sup>Clinical Research and Innovation Unit, Xinhua Hospital Affiliated to Shanghai Jiao Tong University School of Medicine </span>
              </center>
              </td>
      </tr></tbody></table>



          </tr></tbody></table>
          <table align="center" width="850px">
            <tbody><tr>
                    <td align="center" width="850px">
              <center>
                    <span style="font-size:16px"><sup>6</sup>Department of Pediatric Cardiology, Xinhua Hospital Affiliated to Shanghai Jiao Tong University School of Medicine </span>
                </center>
                </td>
          </tr></tbody></table>
   <br>
  <table align=center width=500px>
        <tr>
          <td align=center width=500px>
            <center>
              <span style="font-size:22px">
                <span style="font-size:20px">Preview</span>
              </span>
            </center>
          </td>
        </tr>
      </table> 
  <br><hr>
  <table align="center" width="750px">
            <tbody><tr>
              <td align="center" width="180px">
                <center>
                  <br>
                  <span style="font-size:17px">Code
                    <a href="https://github.com/MAGIC-AI4Med/KEEP"> [GitHub]</a>
                  </span>
                </center>
              </td>

              <td align="center" width="180px">
                <center>
                  <br>
                  <span style="font-size:17px">
                    Paper <a href="https://arxiv.org/abs/2412.13126"> [arXiv]</a>
<!--                     Paper  -->
                  </span>
                </center>
              </td>

              <td align="center" width="180px">
                <center>
                  <br>
                  <span style="font-size:17px">
                    Cite <a href="./resources/bibtex.txt"> [BibTeX]</a>
<!--                     Cite  -->
                  </span>
                </center>
              </td>

              <td align="center" width="210px">
                <center>
                  <br>
                  <span style="font-size:17px">
                    Models <a href="https://huggingface.co/Astaxanthin/KEEP"> [HuggingFace]</a>
                  </span>
                </center>
              </td>
            </tr></tbody>
      </table>
  
      <br><hr>
      
        <!-- <p style="text-align:justify; text-justify:inter-ideograph;"><left>
          <div class="container">
        <div class="image" width="500px">
          <center><p><img class="center" src="./resources/analysis.png" width="450px"></p></center>
        </div>
          <p style="text-align:justify; text-justify:inter-ideograph;"><left></left>
          We present an innovative and <strong>automatic</strong> audio caption generation pipeline(<strong>*</strong>), construct a large-scale, high-quality, audio-language dataset, named as <strong>Auto-ACD</strong>, comprising over <strong>1.9M </strong> audio-text pairs. 
          As shown in the left figure, The text descriptions in Auto-ACD contain <strong>long texts (18 words)</strong> and <strong>diverse vocabularies (23K)</strong>, and provide information about the <strong>surrounding auditory environment</strong>(data point with <strong>shadow</strong>) in which sounds take place.
        </left></p>  
      </div>
      </left></p>
      <hr>
      <br> -->



      <center><h2> Abstract </h2> </center>
      <p style="text-align:justify; text-justify:inter-ideograph;">
      </p><div class="container">
        <div class="text" width="400px"> 
          <p style="text-align:justify; text-justify:inter-ideograph;">
            <left>
              Deep learning has enabled the development of highly robust foundation models for various pathological tasks across diverse diseases and patient cohorts. 
              Among these models, vision-language pre-training, which leverages large-scale paired data to align pathology image and text embedding spaces, and provides a novel zero-shot paradigm for downstream tasks. 
              However, existing models have been primarily data-driven and lack the incorporation of domain-specific knowledge, which limits their performance in cancer diagnosis, especially for rare tumor subtypes. 
              To address this limitation, we establish a <b>K</b>nowledg<b>E</b>-<b>E</b>nhanced <b>P</b>athology (<b>KEEP</b>) foundation model that harnesses disease knowledge to facilitate vision-language pre-training. 
              Specifically, we first construct a disease knowledge graph (KG) that covers 11,454 human diseases with 139,143 disease attributes, including synonyms, definitions, and hypernym relations. 
              We then systematically reorganize the millions of publicly available noisy pathology image-text pairs, into 143K well-structured semantic groups linked through the hierarchical relations of the disease KG. 
              To derive more nuanced image and text representations, we propose a novel knowledge-enhanced vision-language pre-training approach that integrates disease knowledge into the alignment within hierarchical semantic groups instead of unstructured image-text pairs. 
              Validated on 18 diverse benchmarks with more than 14,000 whole slide images (WSIs), KEEP achieves state-of-the-art performance in zero-shot cancer diagnostic tasks. Notably, 
              for cancer detection, KEEP demonstrates an average sensitivity of 89.8% at a specificity of 95.0% across 7 cancer types, significantly outperforming vision-only foundation models and highlighting its promising potential for clinical application. 
              For cancer subtyping, KEEP achieves a median balanced accuracy of 0.456 in subtyping 30 rare brain cancers, indicating strong generalizability for diagnosing rare tumors. All codes and models will be available for reproducing our results.
          </left></p>
        </div>
      </div>
      <br>
      <hr>
      <br>

      <center> <h2> Overview </h2> </center>
      <p><img class="left" src="./resources/teaser.png" width="800px"></p>
      <p style="text-align:justify; text-justify:inter-ideograph;">
        <b>Overview of KEEP.</b> a. Example disease structure in the constructed knowledge graph. Each node represents a
        disease, consisting of three attributes types: hierarchical relations, synonyms, and definitions, as indicated by the dashed line box.
        b. The knowledge encoding and vision-language alignment stage for the KEEP model. A BERT-based text encoder is initially
        trained to encode the disease knowledge through metric learning. A knowledge-enhanced vision-language pre-training approach is
        proposed to align pathology semantic groups with filtered images and augmented captions. c. For downstream cancer diagnostic
        tasks, including cancer region segmentation, cancer detection, and cancer subtyping, whole slide images (WSIs) are divided
        into tile images for zero-shot classification, with the results of each tile combined to determine the final diagnostic decision. d.
        Performance comparison of cancer diagnosis with the state-of-the-art methods on 18 benchmarks with more than 14,000 WSIs. e.
        Performance comparison of tile-level classification with the state-of-the-art methods on 14 benchmarks. The inner and outer
        numbers indicate the worst and best results, respectively.</p>
        <br>
        <hr>
        <br>

      <center> <h2> Protocol-I: Zero-shot Cancer Region Segmentation </h2> </center>
      <p><img class="left" src="./resources/segmentation_results.png" width="800px"></p>
      <p style="text-align:justify; text-justify:inter-ideograph;">
        <b>Statistics of semantic groups and zero-shot cancer region segmentation results.</b> a. Statistics of all
        semantic groups, organized by structuring one million noisy pathology image-text pairs with the guidance of disease KG. More
        than 60% semantic groups are linked to specific disease nodes. b. The anatomy and cell type distribution of the semantic groups
        with known disease labels, where "other" denotes the anatomy or cell type remains unknown. The anatomical taxonomy is
        based on OncoTree. The anatomy and tumor types with the largest number of semantic groups are skin and carcinoma,
        respectively. c. The scheme of zero-shot segmentation on WSIs, where individual tiles undergo binary classification and are then
        combined to delineate the cancerous region. d,e. Performance comparisons of AUROC and DICE scores for various models,
        including PLIP, QuiltNet, MI-Zero, CONCH, and our proposed KEEP, across three WSI datasets: CAMELYON16 (48 WSIs),
        PANDA (10,494 WSIs), and AGGC22 (128 WSIs). The box plots present the median, first, and third quartiles of results, with μ
        indicating the average performance. The DICE is calculated using the average threshold corresponding to the optimal cutoff
        point of ROC curves in each dataset. Our proposed model, KEEP, achieves the best DICE and AUROC performance across all
        WSI datasets compared to other state-of-the-art models. f. Example WSIs from three datasets (the first two for CAMELYON16,
        the middle two for PANDA, and the last two for AGGC22) showing ground truth and predicted segmentation masks.
      <br>
      <hr>
      <br>
      <center> <h2> Protocol-II: Zero-shot Cancer Detection </h2> </center>
      <p><img class="left" src="./resources/segmentation_results.png" width="800px"></p>
      <p style="text-align:justify; text-justify:inter-ideograph;">
        <b>Zero-shot cancer detection results.</b> a. The zero-shot cancer detection scheme on WSIs, where individual tiles
        undergo binary classification. The probability of a slide being cancerous is determined by the predicted tumor ratio which is
        calculated by the ratio of tumor tiles to all valid tiles. b. The comparison of the predicted tumor ratio between normal and
        cancer WSIs in CPTAC-CM and CPTAC-CCRCC datasets. Two-sided Welch's <i>t</i> test is used to assess the statistical significance
        of predicted tumor ratios among different WSIs. c-i. Comparison of ROC curves across different models, including CHIEF, PLIP,
        QuiltNet, MI-Zero, CONCH, and KEEP, evaluated on seven CPTAC datasets across six tissue anatomies: skin, kidney, pancreas,
        uterine, lung, and head and neck. Each dataset consists of 75 normal WSIs and 75 cancer slides, with each experiment using
        1,000 bootstrap iterations. The AUROC for each model is reported as the median along with its 95% confidence intervals (CIs).
        j. Comparison of average sensitivities across all datasets at the specificity of 0.95, the error bar denotes the standard deviation of
        the performance. k. The robustness of our approach towards the threshold of the zero-shot classifier. l,m. Example visualizations
        of cancer detection on CPATC-CM, CPTAC-UCEC datasets. The first and the second rows denote the normal and the cancer
        WSIs. The heat map is generated by the similarities between the embeddings of tile images and that of "tumor" prompts.
      <br>
      <hr>
      <br>

      <center> <h2> Protocol-III: Zero-shot Cancer Subtyping </h2> </center>
      <p><img class="left" src="./resources/subtyping_results.png" width="800px"></p>
      <p style="text-align:justify; text-justify:inter-ideograph;">
        <b>Zero-shot cancer subtyping results.</b> a. The zero-shot cancer subtyping scheme on WSIs, where individual
        tiles undergo multi-class classification, including a "normal" label and tumor subtype labels. The probability of a slide being
        classified as type I is determined by the ratio of type I tiles to all valid tiles. b. Comparison of balanced accuracy across
        different models on seven datasets with common cancer subtypes. The TCGA-BRCA, TCGA-NSCLC, TCGA-ESCA, and
        CPTAC-NSCLC datasets contain two subtypes, while the TCGA-RCC, TCGA-BRAIN, and UBC-OCEAN datasets consist of 3,
        3, and 5 subtypes, respectively. Each subtype includes 75 WSIs, except for TCGA-ESCA (65 WSIs) and UBC-OCEAN (35
        WSIs), with each experiment using 1,000 bootstrap iterations. c. Performance comparison of different models on the rare cancer
        subtyping dataset, EBRAINS, which consists of 30 rare brain cancer subtypes, each with 30 WSIs. d. The confusion matrix of
        the KEEP model on the rare brain cancer dataset, EBRAINS. e. Ablation results. Performance comparison between simple
        contrastive (Contrastive-Top100), KEEP with knowledge enhancement (KEEP-Top100), and KEEP with tumor-ratio strategy
        (KEEP-Ratio). Top100 suggests the strategy of top-100 pooling, while Ratio denotes the subtype ratio strategy. f. Example WSIs
        for tumor subtyping. The left and the right WSIs denote esophagus adenocarcinoma and esophagus squamous cell carcinoma,
        respectively. The orange and the green masks denote the predicted regions of adenocarcinoma and squamous cell carcinoma,
        respectively. The blue squares denote the tile image from the area with normal predictions.
      <br>
      <hr>
      <br>

      <center> <h2> Protocol-IV: Zero-shot Pathology Tile Image Profiling </h2> </center>
      <p><img class="left" src="./resources/tile_results.png" width="800px"></p>
      <p style="text-align:justify; text-justify:inter-ideograph;">
        Zero-shot tile image profiling results. a. Performance comparison of different models on the cross-modal
        retrieval task. R@K (k = 5, 10, and 50) denotes Recall@K, the ratio of correctly retrieved queries in Top-K retrieved samples. b.
        Performance comparison of different models on the zero-shot tile image classification task. The error bar denotes the standard
        deviation of the results from 1000 bootstrapping iterations
      <br>
      <hr>
      <br>


      <center> <h2> Methods </h2> </center>
      <p><img class="left" src="./resources/methods_fig.png" width="800px"></p>
      <p style="text-align:justify; text-justify:inter-ideograph;">
        <b>Architecture of KEEP.</b> a. Disease knowledge encoding. We establish a knowledge graph that includes hypernym
        relations, synonyms, and definitions of diseases, and pre-trained a disease knowledge encoder. Diseases at different levels are
        represented by different colors. b. Knowledge-guided dataset structuring. We fine-tune YOLOv8 to remove noise in the pathology
        image dataset, extract medical entities from the captions, align the diseases in the captions with the diseases and synonyms in
        the knowledge graph, and cluster the filtered image and text data into semantic groups. The right side illustrates two specific
        methods used during the clustering process. c. Knowledge-enhanced vision-language pre-training. We perform cropping and
        random dropping augmentations on the images and texts, and paraphrase captions that contain diseases using templates. During
        the training process, to mitigate the impact of false negatives, we design strategies for positive mining, hardest negative, and
        false negative elimination.
      
      <br>
      <hr>
      <br>



      <center> <h2> Acknowledgements </h2> </center>
      <p> 
        Based on a template by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a>.
      </p>
      <br>
<br>


</body><grammarly-desktop-integration data-grammarly-shadow-root="true"></grammarly-desktop-integration></html>
