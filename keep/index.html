

<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<style type="text/css">
  @font-face {
   font-family: 'Avenir Book';
   src: url("./fonts/Avenir_Book.ttf"); /* File to be stored at your site */
   }

  body {
    font-family: "Avenir Book", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight:300;
    font-size:14px;
    margin-left: auto;
    margin-right: auto;
    width: 800px;
  }
  h1 {
    font-weight:300;
  }
  h2 {
    font-weight:300;
  }

  p {
    font-weight:300;
    line-height: 1.4;
  }

  code {
    font-size: 0.8rem;
    margin: 0 0.2rem;
    padding: 0.5rem 0.8rem;
    white-space: nowrap;
    background: #efefef;
    border: 1px solid #d3d3d3;
    color: #000000;
    border-radius: 3px;
  }

  pre > code {
    display: block;
    white-space: pre;
    line-height: 1.5;
    padding: 0;
    margin: 0;
  }

  pre.prettyprint > code {
    border: none;
  }


  .container {
        display: flex;
        align-items: center;
        justify-content: center
  }
  .image {
        flex-basis: 40%
  }
  .text {
        padding-left: 20px;
        padding-right: 20px;
  }
  .grid-container {
      display: grid;
      grid-template-columns: auto auto auto;
      grid-gap: 10px;
      background-color: #FFFFFF;
      padding: 10px;
}

  .disclaimerbox {
    background-color: #eee;
    border: 1px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
    padding: 20px;
  }

  video.header-vid {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.header-img {
    height: 140px;
    border: 1px solid black;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;
  }

  img.rounded {
    border: 0px solid #eeeeee;
    border-radius: 10px ;
    -moz-border-radius: 10px ;
    -webkit-border-radius: 10px ;

  }

  a:link,a:visited
  {
    color: #1367a7;
    text-decoration: none;
  }
  a:hover {
    color: #208799;
  }

  td.dl-link {
    height: 160px;
    text-align: center;
    font-size: 22px;
  }

  .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
            15px 15px 0 0px #fff, /* The fourth layer */
            15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
            20px 20px 0 0px #fff, /* The fifth layer */
            20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
            25px 25px 0 0px #fff, /* The fifth layer */
            25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
    margin-left: 10px;
    margin-right: 45px;
  }


  .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
    box-shadow:
            0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
            5px 5px 0 0px #fff, /* The second layer */
            5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
            10px 10px 0 0px #fff, /* The third layer */
            10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
    margin-top: 5px;
    margin-left: 10px;
    margin-right: 30px;
    margin-bottom: 5px;
  }

  .vert-cent {
    position: relative;
      top: 50%;
      transform: translateY(-50%);
  }

  hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }
</style>

<title>KEEP</title>
</head>

<body data-new-gr-c-s-check-loaded="14.1093.0" data-gr-ext-installed="">
  <br>
  <center>
    <span style="font-size:32px">A Knowledge-enhanced Pathology Vision-language Foundation Model for Cancer Diagnosis</span><br><br><br>
  </center>
  <table align="center" width="800px">
    <tbody>
      <tr>
        <td align="center" width="800px">
          <center>
            <span style="font-size:16px">
              Xiao Zhou<sup>1</sup>, Luoyi Sun<sup>1,2</sup>, Dexuan He<sup>3</sup>, Wenbin Guan<sup>4</sup>,
              Ge Wang<sup>5</sup>, Ruifen Wang<sup>4</sup>, Lifeng Wang<sup>4</sup>
            </span>
          </center>
        </td>
      </tr>
      <tr>
        <td align="center" width="800px">
          <center>
            <span style="font-size:16px">
              Xiaojun Yuan<sup>6</sup>, Xin Sun<sup>7</sup>, Ya Zhang<sup>1,3</sup>, Kun Sun<sup>8</sup><img class="round" style="width:13px" src="./resources/corresponding_fig.png">,
              Yanfeng Wang<sup>1,3</sup><img class="round" style="width:13px" src="./resources/corresponding_fig.png">, Weidi Xie<sup>1,3</sup><img class="round" style="width:13px" src="./resources/corresponding_fig.png">
            </span>
          </center>
        </td>
      </tr>
    </tbody>
  </table>
  <br>
  
  <table align="center" width="800px">
    <tbody>
      <tr>
        <td align="center" width="800px">
          <center>
            <span style="font-size:13px">
              <sup>1</sup> Shanghai Artificial Intelligence Laboratory, <sup>2</sup> Zhejiang University
            </span>
          </center>
        </td>
      </tr>
      <tr>
        <td align="center" width="800px">
          <center>
            <span style="font-size:13px">
              <sup>3</sup> School of Artificial Intelligence, Shanghai Jiao Tong university
            </span>
          </center>
        </td>
      </tr>
      <tr>
        <td align="center" width="800px">
          <center>
            <span style="font-size:13px">
              <sup>4</sup> Department of Pathology, Xin Hua Hospital Affiliated to Shanghai Jiao Tong University School of Medicine
            </span>
          </center>
        </td>
      </tr>
      <tr>
        <td align="center" width="800px">
          <center>
            <span style="font-size:13px">
              <sup>5</sup> Department of Oral Pathology, Shanghai Ninth People’s Hospital, Shanghai Jiao Tong University School of Medicine
            </span>
          </center>
        </td>
      </tr>
      <tr>
        <td align="center" width="800px">
          <center>
            <span style="font-size:13px">
              <sup>6</sup> Department of Pediatric Hematology/Oncology, Xinhua Hospital Affiliated to Shanghai Jiao Tong University School of Medicine
            </span>
          </center>
        </td>
      </tr>
      <tr>
        <td align="center" width="800px">
          <center>
            <span style="font-size:13px">
              <sup>7</sup> Clinical Research and Innovation Unit, Xinhua Hospital Affiliated to Shanghai Jiao Tong University School of Medicine
            </span>
          </center>
        </td>
      </tr>
      <tr>
        <td align="center" width="800px">
          <center>
            <span style="font-size:13px">
              <sup>8</sup> Department of Pediatric Cardiology, Xinhua Hospital Affiliated to Shanghai Jiao Tong University School of Medicine
            </span>
          </center>
        </td>
      </tr>
    </tbody>
  </table>

  <!-- <br> -->

  <!-- <table align=center width=500px>
    <tr>
      <td align=center width=500px>
        <center>
          <span style="font-size:20px">
            Under Review in Nature Cancer
          </span>
      </td>
    </tr>
  </table>  -->
  <br>
  <hr>
  <table align="center" width="750px">
    <tbody>
      <tr>
        <td align="center" width="180px">
          <center>
            <br>
            <span style="font-size:17px">
              Code<a href="https://github.com/MAGIC-AI4Med/KEEP"> [GitHub]</a>
            </span>
          </center>
        </td>

        <td align="center" width="180px">
          <center>
            <br>
            <span style="font-size:17px">
              Paper <a href="https://arxiv.org/abs/2412.13126"> [arXiv]</a>
            </span>
          </center>
        </td>

        <td align="center" width="180px">
          <center>
            <br>
            <span style="font-size:17px">
              Cite <a href="./resources/bibtex.txt"> [BibTeX]</a>
            </span>
          </center>
        </td>

        <td align="center" width="210px">
          <center>
            <br>
            <span style="font-size:17px">
              Dataset <a href="https://huggingface.co/datasets/Loie/KEEP_dataset"> [HuggingFace]</a>
            </span>
          </center>
        </td>
      </tr>
    </tbody>
  </table>
  
  <br>
  <hr>

  <center><h2> Overview </h2></center>
  <p>
    <img class="left" src="./resources/teaser.png" width="800px">
  </p>
  <p style="text-align:justify; text-justify:inter-ideograph;"><left>
    <strong>Overview of KEEP. </strong>
    <strong>a.</strong> Example disease structure in the constructed knowledge graph. Each node represents a disease, consisting of three attribute types: hierarchical relations, synonyms, and definitions, as indicated by the dashed line box. 
    <strong>b.</strong> The knowledge encoding and vision-language alignment stage for the KEEP model. A BERT-based text encoder is initially trained to encode the disease knowledge through metric learning. A knowledge-enhanced vision-language pre-training approach is proposed to align pathology semantic groups with filtered images and augmented captions. 
    <strong>c.</strong> For downstream cancer diagnostic tasks, including cancer region segmentation, cancer detection, and cancer subtyping, whole slide images (WSIs) are divided into tile images for zero-shot classification, with the results of each tile combined to determine the final diagnostic decision. The text prompt for zero-shot classification is [template + disease name], for instance, A histopathology image of lung adenocarcinoma. More templates can be found in Supplementary Table S19. 
    <strong>d.</strong> The flowchart of cancer diagnosis, including WSI pre-processing and titling, Tile-level processing through KEEP model, and mapping and aggregation of predictions. 
    <strong>e.</strong> Performance comparison of cancer diagnosis with the state-of-the-art methods on 18 public benchmarks of more than 14,000 WSIs. 
    <strong>f.</strong> Performance comparison of tile-level classification with the state-of-the-art methods on 14 benchmarks. The inner and outer numbers indicate the worst and best results, respectively.
  </p>

  <br>
  <hr>
  <br>

  <center><h2> Abstract </h2></center>
  <p style="text-align:justify; text-justify:inter-ideograph;"></p>
  <div class="container">
    <div class="text" width="400px"> 
      <p style="text-align:justify; text-justify:inter-ideograph;">
        Deep learning has enabled the development of highly robust foundation models for various pathological tasks across diverse diseases and patient cohorts. 
        Among these models, vision-language pre-training, which leverages large-scale paired data to align pathology image and text embedding spaces, and provides a novel zero-shot paradigm for downstream tasks. 
        However, existing models have been primarily data-driven and lack the incorporation of domain-specific knowledge, which limits their performance in cancer diagnosis, especially for rare tumor subtypes. 
        To address this limitation, we establish a <strong>K</strong>nowledg<strong>E</strong>-<strong>E</strong>nhanced <strong>P</strong>athology (<strong>KEEP</strong>) foundation model that harnesses disease knowledge to facilitate vision-language pre-training. 
        Specifically, we first construct a disease knowledge graph (KG) that covers 11,454 human diseases with 139,143 disease attributes, including synonyms, definitions, and hypernym relations. 
        We then systematically reorganize the millions of publicly available noisy pathology image-text pairs, into 143K well-structured semantic groups linked through the hierarchical relations of the disease KG. 
        To derive more nuanced image and text representations, we propose a novel knowledge-enhanced vision-language pre-training approach that integrates disease knowledge into the alignment within hierarchical semantic groups instead of unstructured image-text pairs. 
        Validated on 18 diverse public benchmarks with more than 14,000 whole slide images (WSIs) and 3 in-house rare cancer datasets with 816 WSIs, KEEP achieves state-of-the-art performance in zero-shot cancer diagnostic tasks. 
        Notably, for cancer detection, KEEP demonstrates an average sensitivity of 89.8% at a specificity of 95.0% across 7 cancer types, significantly outperforming vision-only foundation models and highlighting its promising potential for clinical application. 
        For cancer subtyping, KEEP achieves a median balanced accuracy of 0.456 in subtyping 30 rare brain cancers, indicating strong generalizability for diagnosing rare tumors. 
        All codes and models will be available for reproducing our results.
      </p>
    </div>
  </div>

  <br>
  <hr>
  <br>



  <center><h2>Protocol-I: Zero-shot Cancer Region Segmentation</h2></center>
  <p style="text-align:justify; text-justify:inter-ideograph;">
    <strong>a.</strong> The scheme of zero-shot segmentation on WSIs, where individual tiles undergo binary classification and are then combined to delineate the cancerous region. 
    <strong>b,c.</strong> Performance comparisons of AUROC and DICE scores for various models, including PLIP, QuiltNet, MI-Zero, CONCH, and MUSK, and our proposed KEEP, across three WSI datasets: CAMELYON16(48 WSIs), PANDA(10,494 WSIs), and AGGC22(128 WSIs). 
    The box plots present the median, first, and third quartiles of results, with μ indicating the average performance. The DICE is calculated using the average threshold corresponding to the optimal cutoff point of ROC curves in each dataset. 
    KEEP-Post denotes the segmentation results produced by the KEEP algorithm after post-processing with a morphological opening operation, which removes small noisy regions while preserving the shape of larger structures. 
    <strong>d.</strong> Exemplary WSIs from three datasets (the first two for CAMELYON16, the middle two for PANDA, and the last two for AGGC22) showing ground truth and predicted segmentation masks. 
    The number in the top-left of each result image suggests the DICE score.
  </p>
  <p><img class="center" src="./resources/seg.png" width="800px"></p>

  <br>
  <hr>
  <br>


  <center><h2>Protocol-II: Zero-shot Cancer Detection</h2></center>
  <p style="text-align:justify; text-justify:inter-ideograph;">
    <strong>a.</strong> The zero-shot cancer detection scheme on WSIs, where individual tiles undergo binary classification. 
    The probability of a slide being cancerous is determined by the predicted tumor ratio which is calculated by the ratio of tumor tiles to all valid tiles. 
    <strong>b.</strong> The comparison of the predicted tumor ratio between normal and cancer WSIs in CPTAC-CM and CPTAC-CCRCC datasets. 
    Two-sided Welch’s t test is used to assess the statistical significance of predicted tumor ratios among different WSIs. 
    <strong>c-i.</strong> Comparison of ROC curves across different models, including CHIEF, PLIP, QuiltNet, MI-Zero, CONCH, and MUSK and KEEP, evaluated on 7 CPTAC datasets across 6 tissue anatomies: skin, kidney, pancreas, uterine, lung, and head and neck. Each dataset consists of 75 normal WSIs and 75 cancer slides, with each experiment using 1,000 bootstrap iterations. 
    The AUROC for each model is reported as the median along with its 95% confidence intervals (CIs). 
    <strong>j.</strong> Comparison of average sensitivities across all datasets at the specificity of 0.95, the error bar denotes the standard deviation of the performance. 
    <strong>k.</strong> The robustness of our approach towards the threshold of the zero-shot classifier. l,m. Example visualizations of cancer detection on CPATC-CM, CPTAC-UCEC datasets. 
    The first and the second rows denote the normal and the cancer WSIs. The heat map is generated by the similarities between the embeddings of tile images and those of “tumor” prompts.
  </p>
  <p><img class="center" src="./resources/detection.png" width="800px"></p>

  <br>
  <hr>
  <br>


  <center><h2>Protocol-III: Zero-shot Cancer Subtyping</h2></center>
  <p style="text-align:justify; text-justify:inter-ideograph;">
    <strong>a.</strong> The zero-shot cancer subtyping scheme on WSIs, where individual tiles undergo multi-class classification, including a “normal” label and tumor subtype labels. 
    The probability of a slide being classified as type I is determined by the ratio of type I tiles to all valid tiles. 
    <strong>b.</strong> Comparison of balanced accuracy across different models on seven datasets with common cancer subtypes. 
    The TCGA-BRCA, TCGA-NSCLC, TCGA-ESCA, and CPTAC-NSCLC datasets contain two subtypes, while the TCGA-RCC, TCGA-BRAIN, and UBC-OCEAN datasets consist of 3, 3, and 5 subtypes, respectively. Each subtype includes 75 WSIs, except for TCGA-ESCA (65 WSIs) and UBC-OCEAN (35 WSIs), with each experiment using 1,000 bootstrap iterations. 
    <strong>c.</strong> Performance comparison of different models on the rare cancer subtyping dataset, EBRAINS, which consists of 30 rare brain cancer subtypes, each with 30 WSIs. 
    <strong>d.</strong> The confusion matrix of the KEEP model on the rare brain cancer dataset, EBRAINS. 
    <strong>e.</strong> Ablation results on WSI tasks. Performance comparison between naïve contrastive and knowledge-enhanced (KEEP). 
    <strong>f.</strong> Ablation results between naïve contrastive with Top-100 pooling strategy (Contrast-Top100), KEEP with Top-100 pooling strategy (KEEP-top100) and KEEP with tumor-ratio strategy (KEEP-Ratio). 
    <strong>g.</strong> Example WSIs for tumor subtyping. The left and the right WSIs denote esophagus adenocarcinoma and esophagus squamous cell carcinoma, respectively. 
    The orange and the green masks denote the predicted regions of adenocarcinoma and squamous cell carcinoma, respectively. 
    The blue squares denote the tile image from the area with normal predictions.
  </p>
  <p><img class="center" src="./resources/subtyping.png" width="800px"></p>

  <br>
  <hr>
  <br>

  <center><h2>Protocol-IV: Rare Cancer Subtyping and Zero-shot Tile Image Profiling</h2></center>
  <p style="text-align:justify; text-justify:inter-ideograph;">
    Performance comparison of different models on the clinical rare cancer subtyping task in a zero-shot <strong>(a)</strong> and fine-tuning way <strong>(b)</strong>. 
    <strong>c.</strong> Performance comparison of different models on the cross-modal retrieval task. R@K (k = 5, 10, and 50) denotes Recall@K, the ratio of correctly retrieved queries in Top-K retrieved samples. 
    <strong>d.</strong> Performance comparisons of different models on the zero-shot tile image classification task. The error bar denotes the standard deviation of the results from 1000 bootstrapping iterations.
    `Average' suggests the mean of performance across all datasets.
  </p>
  <p><img class="center" src="./resources/rare.png" width="800px"></p>

  <br>
  <hr>
  <br>

  <center> <h2> Architecture </h2> </center>
  <p><img class="left" src="./resources/arch.png" width="800px"></p>
  <p style="text-align:justify; text-justify:inter-ideograph;"><left>
    <strong>Architecture of KEEP. </strong>
    <strong>a.</strong> Disease knowledge encoding. We establish a knowledge graph that includes hypernym relations, synonyms, and definitions of diseases, and pre-train a disease knowledge encoder. 
    Diseases at different levels are represented by different colors. 
    <strong>b.</strong> Knowledge-guided dataset structuring. We fine-tune YOLOv8 to remove noise in the pathology image dataset, extract medical entities from the captions, align the diseases in the captions with the diseases and synonyms in the knowledge graph, and cluster the filtered image and text data into semantic groups. 
    The right side illustrates two specific methods used during the clustering process. 
    <strong>c.</strong> Knowledge-enhanced vision-language pre-training. 
    We perform cropping and random dropping augmentations on the images and texts, and paraphrase captions that contain diseases using templates. 
    During the training process, to mitigate the impact of false negatives, we design strategies for positive mining, hardest negative, and false negative elimination.

  <br>
  <hr>
  <br>

  <center> <h2> Discussion </h2> </center>
  <!-- <p><img class="left" src="./resources/arch.png" width="800px"></p> -->
  <p style="text-align:justify; text-justify:inter-ideograph;"><left>
    In this study, we present KEEP, a novel vision-language foundation model specifically designed to tackle challenges in computational pathology. 
    By incorporating disease-specific knowledge, KEEP achieves state-of-the-art performance in zero-shot cancer diagnosis. 
    In particular, for cancer detection, KEEP significantly outperforms CHIEF, achieving a notable improvement in sensitivity (0.898) at a specificity of 0.95 across multiple cancer types. 
    Similarly, in cancer subtyping, KEEP outperforms CONCH and MUSK by integrating disease-specific knowledge, which improves the alignment between pathology images and subtype semantics.
    Notably, in rare cancer subtyping tasks, KEEP achieves a balanced accuracy improvement of 8.5 points over CONCH. 
    Furthermore, when applied to in-house rare pediatric cancer subtyping tasks, KEEP outperforms other foundation models in the neuroblastoma and hepatoblastoma datasets.
    <br>
    <br>


    The superior performance of KEEP compared to other models can be primarily attributed to two factors: 
    the injection of disease knowledge during training and the tumor-ratio-based prediction strategy employed in downstream tasks: 
    (i) integration of disease knowledge during training: disease knowledge is incorporated through three key mechanisms. 
    First, the language model is employed to encode the disease knowledge graph(KG), aligning the representation space of disease names, definitions, synonyms, and hierarchical relationships.
    This alignment serves as a bridge, implicitly linking pathology images with their corresponding disease types during the vision-language pre-training process. 
    Second, the extensive use of disease synonyms within the disease KG explicitly highlights critical disease-related information in the textual descriptions of images.
    This transformation converts weak supervision signals from free-text annotations into stronger disease-level supervision, thereby reinforcing the connection between pathology images and disease entities. 
    Finally, the hierarchical structure of the disease KG organizes image-text pairs into semantic groups with hypernym-hyponym relationships, significantly enhancing alignment accuracy. 
    (ii) tumor-ratio-based prediction in downstream tasks: for both cancer detection and subtyping, mathematical analysis and experimental results demonstrate that the tumor-ratio-based prediction serves as an intuitive and effective way for estimating the neoplastic and subtyping probability of WSIs. 
    Unlike methods that aggregate tile-level features, the tumor-ratio-based approach leverages tumor region localization to deliver superior interpretability, a critical requirement for clinical applications. 
    Moreover, compared to approaches that predict tumor subtypes by selecting top-k tumor patches, the tumor-ratio-based method is non-parametric, offering a more straightforward and transparent classification process.
    <br>
    <br>

    While promising, this study also faces several limitations: 
    (i) although KEEP demonstrates strong performance in rare cancer subtyping, its predictions for certain subtypes remain limited due to the scarcity of these cases in the image-text training data. 
    In these instances, few-shot learning, where at least one whole slide image(WSI) per subtype is available, could enhance model performance by capturing the diversity within rare cancer subtypes. 
    (ii) while pathology vision-language models exhibit robust zero-shot ability, their performance is often dependent on prompt engineering, which can limit their adaptability and robustness across diverse datasets. 
    A promising avenue for improvement is prompt learning, wherein the model learns a trainable prompt from a small set of example WSIs, replacing manually designed prompts. 
    This approach would enable the model to adapt more effectively to varied datasets and tasks, enhancing its generalizability and robustness.
    (iii) another promising avenue for improvement is the multi-modal alignment that introduces genomic, or epigenomic information.
    <br>
    <br>

    In conclusion, our results demonstrate that KEEP offers a powerful tool for cancer diagnosis by injecting domain-specific knowledge into vision-language models. 
    KEEP shows great promise in advancing computational pathology and has the potential to make a significant impact in clinical settings, offering improved accuracy and interpretability in cancer diagnosis.

  <br>
  <hr>
  <br>
  <center> <h2> Acknowledgements </h2> </center>
  <p> 
    Based on a template by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a>.
  </p>
  <br>
</body><grammarly-desktop-integration data-grammarly-shadow-root="true"></grammarly-desktop-integration></html>
